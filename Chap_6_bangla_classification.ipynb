{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Chap_6_bangla_classification-final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raqueeb/nlp_bangla/blob/master/Chap_6_bangla_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJrBxVCUF_hJ"
      },
      "source": [
        "### বাংলা ক্ল্যাসিফিকেশন, ‘জেসন’ ফরম্যাটের ডাটাসেট\n",
        "\n",
        "আপনারা কি একটা জিনিস লক্ষ্য করেছেন - আমরা যখন ইন্টারনেটে প্রচুর ওয়েবসাইটের ডাটাবেজে এক্সেস করি, সেগুলোর বেশিরভাগই এখন মাইক্রো সার্ভিসে চলে এসেছে। এটা ঠিক যে - পৃথিবীতে যেখানে বেশিরভাগ ডেটা ওয়েবের মাধ্যমে অ্যাক্সেস করা হয়, তার বেশিরভাগ ডাটা প্রায়শই আমরা JSON ফর্ম্যাটে দেখতে পাচ্ছি। বিভিন্ন ওয়েব সার্ভিসের ‘এপিআই’ থেকে ডাটা টানার সময় আমাদের এই ধরনের ফরম্যাটের ডাটা প্রয়োজন হতে পারে, যার স্ট্রাকচার খুবই সহজ এবং কোন ধরনের আলাদা পার্সার ব্যবহার ছাড়াই এটাকে পড়া সম্ভব। \n",
        "\n",
        "আমরা যখন ‘এক্সএমএল’ ডাটা নিয়ে কাজ করতাম, সেটা ওরকম ‘হিউম্যান রিডেবল’ নয় এবং এর জন্য আলাদা করে ‘পার্সার’ মানে সফটওয়্যার লাগতো যা আমাদের জন্য বেশ সমস্যাজনক বটে। JSON এর অর্থ ‘জাভাস্ক্রিপ্ট অবজেক্ট নোটেশন’। ‘জেসন’ হ'ল খুবই ‘লাইটওয়েট’ ডাটা ফর্ম্যাট যা বিভিন্ন ভাষা অর্থাৎ ল্যাঙ্গুয়েজের মধ্যে ডাটা ইন্টারচেঞ্জের জন্য ব্যবহৃত হয়। এটা মানুষের পক্ষে পড়া সহজ এবং যেকোন মেশিন দ্বারা সহজেই ‘পার্স’ করা যায়।\n",
        "\n",
        "### ইন্ডাস্ট্রি স্ট্যান্ডার্ড এপিআই\n",
        "\n",
        "এর আগের যতগুলো বই লিখেছি সেই বইগুলোতে আমি ডাটাসেট হিসেবে সাধারণত “কমা সেপারেটেড ভ্যালু” অথবা ‘সিএসভি’ ফরম্যাট ব্যবহার করেছি। ‘সিএসভি’ ডাটাসেট হিসেবে বড় বড় মেশিন লার্নিং শেখানোর প্ল্যাটফর্ম যেমন ক্যাগলে এখনো ব্যবহার হয় তবে - ইন্ডাস্ট্রিতে কমপ্লেক্স ডাটা চলে আসায় তার জন্য নতুন কিছু ফরম্যাটের প্রয়োজন পড়েছে। সেই কারণে এই বইতে নতুন একটা ফরম্যাট নিয়ে কাজ করতে চাই যা আমার অভিজ্ঞতায় পুরো পৃথিবীর ডাটা ইন্ডাস্ট্রিগুলো ব্যবহার করছে। \n",
        "\n",
        "আমি নিজেও আমার অফিসের কাজে বহুদিন ধরে এই ফরম্যাট ব্যবহার করছি কারণ শুরুতেই এর ব্যবহারের সহজতা এবং ভিন্ন ভিন্ন সিস্টেমের মধ্যে ডাটা ‘ইন্টার এক্সচেঞ্জের’ জন্য এই ফরম্যাটকে সবাই ভালবাসে। সবচেয়ে বড় কথা হচ্ছে - আমরা মানুষেরা এটা সহজেই পড়তে পারি এবং এর মধ্যে যদি কোন কিছু ‘ব্রোকেন’ অর্থাৎ কাজ করছে না তাহলে সেটাকে ‘ডিবাগ’ করা অনেক সহজ।\n",
        "\n",
        "## ডাটার সহজবোধ্যতা, ইন্টিগ্রিটি ঠিক রাখা\n",
        "\n",
        "মনে রাখতে হবে, আমরা যে কাজ করছি সেখানে ডাটা হ্যান্ডলিংয়ের সময় ডাটার অখন্ডতা অর্থাৎ সেটার ‘ইন্টেগ্রিটি’ ধরে রাখতে চাই যাতে ডাটাগুলো এক সিস্টেম থেকে আরেক সিস্টেমে গেলে সেই ডাটাগুলো ‘পাঠযোগ্য’ থাকে। আমি চাইবো যে পুরো মেশিন লার্নিং এর পাইপলাইন ধরে একেকটা ট্রান্সফরমেশনের আগে এবং পরে ডাটাটা কি হচ্ছে সেটা বোঝানোর জন্য একটা সহজ ‘এপিআই’ ফরম্যাট বেছে নিয়েছি এখানে। \n",
        "\n",
        "জেসন এই লক্ষ্যগুলি অর্জনে খুব কার্যকর ভাবে কাজ করছে, এবং এনকোডিং এর জায়গায় জেসন অন্যান্য স্ট্যান্ডার্ড লাইব্রেরির সাথে দুর্দান্ত কার্যকারিতা দেখায়। আমরা পুরো বই জুড়ে এই স্ট্যান্ডার্ড লাইব্রেরির কার্যকারিতাটি ব্যবহার করব। \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gs9htvM7n_x"
      },
      "source": [
        "# আমরা সবসময় চাইবো টেন্সরফ্লো ২.x ব্যবহার করতে\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnYfizc3rT-6"
      },
      "source": [
        "## ৯টা ক্যাটাগরির ক্লাসিফিকেশন\n",
        "\n",
        "আগের ডিপ লার্নিং বইয়ে বাংলা সেন্টিমেন্ট এনালাইসিস দেখেছি - যেখানে টেন্সরফ্লো দিয়ে ছোট একটা ডাটাসেট থেকে তার প্রেডিকশনগুলো দেখিয়ে ছিলাম। আগের ডিপ লার্নিং বইয়ের ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং অংশে একটা নির্ধারিত বাক্যকে মডেলে পাঠালে সে বাক্যের ভেতরের ‘ধারণাগুলো’ বুঝে বাক্যটা ‘পজিটিভ’ না ‘নেগেটিভ’ এই ব্যাপারটা জানিয়ে দিত।\n",
        "\n",
        "আগের বইয়ে বিশেষ করে ‘হাতেকলমে পাইথন ডিপ লার্নিং’ বইটাতে কিছু বাংলা ডাটাসেট ব্যবহার করেছিলাম যেগুলোকে ‘হোস্ট’ করতে হয়েছিল গিটহাবে। গিটহাবে বড় বড় ফাইল, বিশেষ করে - যেগুলো প্রায় কয়েক গিগাবাইটের ওপরে - সেগুলোকে হোস্ট করার সমস্যা হচ্ছে; প্রথমত: ফাইলগুলোর সাইজ এবং পরবর্তীতে এই বড় বড় ফাইলগুলোকে ‘হোস্ট’ করতে হয় গিটহাব ‘এলএফএস’ অর্থাৎ লার্জ ফাইল সিস্টেম হিসেবে, যা অনেক সময় বিনামূল্যের একাউন্টে ব্যান্ডউইড্থ ট্রাফিকের লিমিটে পড়ে যায়।\n",
        "\n",
        "সেদিক থেকে বড় বড় ডাটাসেট ‘পোষ্ট’ করার জন্য ক্যাগল একটা ভালো প্ল্যাটফর্ম। এরমধ্যে ‘ব্যান্ডউইডথ’ এবং ডাটাসেটের সাইজের উপরে সেরকম সীমাবদ্ধতা নেই। সবচেয়ে বড় কথা হচ্ছে, ক্যাগল থেকে বড় বড় ডাটাসেট গুগল ‘কোলাবে’ ডাউনলোড করতে ‘ট্রানস্ফার স্পিড’ পাওয়া যায় প্রতি সেকেন্ডে ১০০ মেগাবাইট এরও বেশি। এটা একটা বিশাল প্রাপ্তি যখন ‘গুগল কোলাব’ আমরা ব্যবহার করি ১২ ঘন্টার জন্য - বিনামূল্যের একাউন্টে।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2qJ7GnTB05J"
      },
      "source": [
        "## দুটো অথেন্টিকেশন পদ্ধতি, গুগল কোলাব এবং ক্যাগল\n",
        "\n",
        "বড় ফাইল নিয়ে ঝামেলা থেকে মুক্তি দিয়েছে 'ক্যাগল'। পুরো পৃথিবী 'ক্যাগলে' হোস্ট করে তাদের ডাটাসেট। সমস্যা একটাই, ‘কোলাব’ থেকে ডাটাসেট ডাউনলোড করতে হলে সেটাকে দুটো অথেন্টিকেশন পদ্ধতির মাধ্যমে যেতে হয় যা স্ক্রিপ্ট হিসেবে লিখে রাখলে প্রতিবারই ব্যবহার করা যাবে। বিশেষ করে, গুগল কোলাবের (জুপিটার) পাইথন নোটবুক চালানোর সময়। শুরুতে, এই দুটোকে কানেক্ট করে দিতে হবে গুগল কোলাবের সাথে ক্যাগলের একাউন্ট - একটা ‘এপিআই টোকেন’ দিয়ে। এটা ডাউনলোড করার জন্য আমাদেরকে যেতে হবে ক্যাগলের গুগলের অ্যাকাউন্ট পেজে, যেখানে আমরা স্ক্রল-ডাউন করে নেমে যাব ‘এপিআই সেকশনে’। ছবি দেখুন।\n",
        "\n",
        "![alt text](https://github.com/raqueeb/nlp_bangla/raw/master/assets/api.png)\n",
        "\n",
        "ছবি: এপিআই সেকশন, ডাউনলোড হয়ে যাবে kaggle.json ফাইল\n",
        "\n",
        "আমাদের যদি আগে থেকে কোন ‘এপিআই টোকেন’ জেনারেট করা থাকে, সেটাকে বাদ দিতে পারি ‘এক্সপায়ার এপিআই টোকেন’ বাটনে ক্লিক করে। আর সেটা না হলে, আমরা সরাসরি একটা টোকেন জেনারেট করতে পারি ‘ক্রিয়েট নিউ এপিআই টোকেন’ বাটনে ক্লিক করে। বাটনটা ক্লিক করলেই kaggle.json ফাইলটা ডাউনলোড হয়ে যাবে এবং তার উপরে এই ফাইলটা কোথায় রাখতে হবে তার ডাইরেক্টরিটার ব্যাপারে বলে দিচ্ছে ক্যাগল। তৈরি করে নিতে হবে kaggle ফোল্ডার, যেখানে কপি করে নেব kaggle.json ফাইলটাকে।\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j68OdrC5D66Z"
      },
      "source": [
        "## আপলোড করে নেই অথেন্টিকেশন টোকেন\n",
        "\n",
        "নতুন একটা নোটবুক তৈরি না করে এই নোটবুকটাকে গিটহাব থেকে ‘ফর্ক’ করে চালালে এই মুহূর্তে ডাউনলোড করা kaggle.json ফাইলটাকে আপলোড করে নিতে হবে গুগল কোলাবে। আমাদের এই ফাইলটা একটা ‘অথেন্টিকেশন মেকানিজম’ - যার মাধ্যমে গুগল কোলাব একটা ডাটাসেটের ফাইলগুলোকে ডাউনলোড করতে গেলে এই ফাইলের মাধ্যমে সে বুঝে নেয় ফাইলের ডাউনলোড রিকোয়েস্টটা আসলে কোথা থেকে আসছে। এটা একটা ‘সিকিউরিটি’ এবং ‘অ্যাকাউন্টেবিলিটি’ চেক গুগলের জন্য। আপনারা ফাইলটাকে টেক্সট এডিটর দিয়ে খুলে দেখতে পারেন। গুগল কোলাবে ফাইল আপলোড করার জন্য নিচের কমান্ডটা ইমপোর্ট করলে সে ফাইল আপলোড করার একটা অপশন দেখিয়ে দেবে।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkqnn6c9GRQH"
      },
      "source": [
        "## ফাইল আপলোড\n",
        "\n",
        "![alt text](https://github.com/raqueeb/nlp_bangla/raw/master/assets/upload.png)\n",
        "\n",
        "ছবি: গুগল কোলাবের আপলোড প্রম্পট, আপলোড করে দিন kaggle.json ফাইল"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu2MaOHX6RyP",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "531a9071-13f8-434f-f05e-feb41c2ecc90"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-70580324-7088-4beb-9ad5-70e4bbe7ced5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-70580324-7088-4beb-9ad5-70e4bbe7ced5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"raqueeb\",\"key\":\"b3ad7579e04a499df9999c2788aa992b\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiY9HwZdEfnb"
      },
      "source": [
        "## কোলাবের ‘ভার্চুয়াল লিনাক্স’ মেশিনে চালান লিনাক্স কমান্ড\n",
        "\n",
        "গুগল কোলাব যেহেতু একটা ‘ভিএম’ অর্থাৎ ‘ভার্চুয়াল লিনাক্স’ মেশিন - সে কারণে আশ্চর্যবোধক (!) চিহ্ন দিয়ে যেকোনো লিনাক্স কমান্ড চালানো যায় গুগল কোলাবে। এই মুহূর্তে আমরা পাইথন প্যাকেজ ম্যানেজার (পিপ) দিয়ে ইন্সটল করে নেব ক্যাগল মডিউলকে, যাতে ক্যাগল সংক্রান্ত সব ধরনের  কমান্ডগুলো চালাতে পারি আমরা। এখানে ক্যাগল লিস্ট দিয়ে সব ধরনের ডাটা সেটের একটা লিস্ট পাওয়া যাবে।\n",
        "\n",
        "ফাইলটা আপলোড করার পর সেটাকে ক্যাগলের ‘ইন্সট্রাকশন’ অনুযায়ী সেটাকে কপি করে রাখতে হবে ‘ক্যাগল’ ফোল্ডারে। এরপর সিস্টেম সিকিউরিটির জন্য আমরা এই ফাইলটাতে কিছু পারমিশন যোগ করে দেব।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5VvFMvE7av3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda1aba5-8252-4e02-838d-f724fd63ca57"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "# পারমিশন দেখে নিন\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovPRlGAtHWhc"
      },
      "source": [
        "## জেসন ডাটাসেট, ক্যাগল থেকে\n",
        "\n",
        "তবে এখানে একটা বাক্যের সেন্টিমেন্ট অ্যানালাইসিস জন্য 'পজিটিভ’ ‘নেগেটিভ’ - মাত্র দুটো ক্যাটাগরি যার কাজটা কিছুটা সহজ ছিল। তবে, এবার আমরা ডাটাসেট থেকে বাক্যটা কোন ধরনের গ্রুপে পড়ে সে রকম দশটা গ্রুপের মধ্যে একটা গ্রুপের মধ্যে তাকে ফেলতে হবে। এর জন্য প্রথম আলো পত্রিকার পুরনো খবরগুলোকে এক জায়গায় নিয়ে এসে একটা সুন্দর ডাটাসেট বানিয়েছেন জাবির নাবিল। বিশেষ করে জেসন ফরম্যাটে। উনার গিটলিংক হচ্ছে “https://github.com/zabir-nabil/bangla-news-rnn”।\n",
        "\n",
        "আমরা যেহেতু একটা নির্দিষ্ট ডাটাসেট  অর্থাৎ বাংলায় পত্রিকার ডাটাসেট ডাউনলোড করব তার জন্য নিচের কমান্ডটি কাজ করবে। এভাবে আপনি যেকোন ডাটাসেট ডাউনলোড করতে পারবেন। \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSEvkDnp7yFv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eccab4d1-f0e0-45a7-e3cd-69b7dabf791b"
      },
      "source": [
        "!kaggle datasets download -d furcifer/bangla-newspaper-dataset"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading bangla-newspaper-dataset.zip to /content\n",
            "100% 1.03G/1.03G [00:11<00:00, 112MB/s]\n",
            "100% 1.03G/1.03G [00:11<00:00, 98.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9rgCIupE6Kw"
      },
      "source": [
        "## গুগল কোলাবের বামে আছে 'ফাইলস' বাটন\n",
        "\n",
        "গুগল কোলাবের বামে ফাইল এক্সপ্লোরের ক্লাউড ভার্চুয়াল মেশিনে কি কি ফাইল ডাউনলোড হয়েছে তা দেখা যাবে। ডাটাসেটের ডাউনলোড করা আর্কাইভ ফাইলটাকে আমরা আনজিপ অর্থাৎ ‘এক্সট্রাক্ট’ করে নেই।  এর ভেতরে আমরা দুটো ফোল্ডার পেলেও আমাদের কাজের জন্য ‘ডাটা ভার্সন টু’ জেসন ফাইলটা ব্যবহার করব। \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQzmehYm5ho7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b039d24-9fa2-47e8-f099-26a46014a9b7"
      },
      "source": [
        "!unzip bangla-newspaper-dataset.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  bangla-newspaper-dataset.zip\n",
            "  inflating: data/data.json          \n",
            "  inflating: data_v2/data_v2.json    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgL1sP6oFg-x"
      },
      "source": [
        "## জেসন ফরম্যাট থেকে পাইথন লিস্টে\n",
        "\n",
        "আমাদের ডাটাসেট যেহেতু স্টোর করা আছে জেসন ফরম্যাটে, কাজের সুবিধার্থে আমরা এটাকে কনভার্ট করে নিয়ে আসব পাইথনে। আমরা যেহেতু আমাদের মডেলকে প্রথমে ট্রেনিং করাবো, আর সে কারণে আমরা সবগুলো জেসন ফরম্যাটেড ডাটাকে পাইথনে নিয়ে আসব ‘লিস্ট’ হিসাবে। এখানে প্রতিটা জেসন এলিমেন্ট চলে আসবে পাইথন ‘লিস্ট’ এলিমেন্ট হিসেবে - যার ভেতরের তথ্যগুলোকে আমরা এখানে দেখছি স্কয়ার ব্র্যাকেট [] দিয়ে। এই কনভারশনটা কিভাবে করব? পাইথনের ভিতর বিল্ট-ইন জেসন প্যাকেজ অর্থাৎ টুলকিট আছে এই কাজটা করার জন্য। সেটাকে ইমপোর্ট করে নিয়ে আসি শুরুতে। জেসন লাইব্রেরি হিসেবে। এরপর আমাদের বাংলা পত্রিকার ডাটাসেট লোড করবো সেই জেসন লাইব্রেরি দিয়ে। তবে শুরুতে একটু জেসনের কাজ দেখি।\n",
        "\n",
        "## শুরুতেই একটা উদাহরণ, জেসন দিয়ে\n",
        "\n",
        "আমরা মেমোরিতে একটা পাইথনের অবজেক্ট তৈরি করি। ফ্লিটউড ম্যাকের একটা প্রিয় গান।\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrH3wHotq7W7"
      },
      "source": [
        "import json\n",
        "\n",
        "data = {\n",
        "    \"music\": {\n",
        "        \"name\": \"Fleetwood Mac\",\n",
        "        \"Song\": \"Isn't It Midnight\"\n",
        "    }\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYfF1hV43yYt"
      },
      "source": [
        "পাইথনের কনটেক্সট ম্যানেজার দিয়ে আমাদের একটা ফাইলে সেভ করতে হবে - যা ওপেন হবে 'রাইট' মোডে। ফাইলটার নাম দিচ্ছি data_file.json। \n",
        "\n",
        "এখানে dump() দুটো পজিশনাল আর্গুমেন্ট নিচ্ছে (১) ডাটা অবজেক্টটাকে সিরিয়ালাইজড করে (২) ফাইলের মতো অবজেক্টটাকে 'রাইট' করে রাখবে ডিস্কে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fn5HswRrA0r"
      },
      "source": [
        "# পাইথন ডিকশনারিটার মতো জিনিসটাকে সেভ করতে চাই -\n",
        "with open(\"data_file.json\", \"w\") as write_file:\n",
        "    json.dump(data, write_file)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCOghmrQzR1a"
      },
      "source": [
        "# আবার খুলি\n",
        "with open(\"data_file.json\", \"r\") as read_file:\n",
        "    data1 = json.load(read_file)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-fojCjtzvuC",
        "outputId": "502bdb5b-ba35-4ab0-dcb6-cbebec04660b"
      },
      "source": [
        "# ভেতরে দেখি\n",
        "data1"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'music': {'Song': \"Isn't It Midnight\", 'name': 'Fleetwood Mac'}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJkUuLN2qy5_"
      },
      "source": [
        "## আলাদা করে দুটো লিস্ট, ফিচার এবং লেবেল (বাংলা নিউজ ডাটাসেট)\n",
        "\n",
        "ফিরে আসি আমাদের নিউজপেপার ডাটাসেটে। শুরুতে লুপের কাজ। এখানে আমরা লেবেলগুলোর লিস্ট তৈরি করবো যার মধ্যে ‘সেন্টেন্স’ এবং ‘লেবেলস’ থাকছে ট্রেনিং করানোর জন্য। ঠিক ধরেছেন দুটো লিস্ট। \n",
        "\n",
        "পাইথনে জেসন ফাইলটা খুলে ‘সেন্টেন্স’ এবং ‘লেবেলস’ দুটোকে আলাদা আলাদা লিস্টে রাখার জন্য আমরা একটা লুপ চালাবো। তবে, তার আগে ‘ডাটাস্টোর’ বলে অবজেক্টে সবকিছু রাখার জন্য আগে ডিক্লেয়ার করে নেব। শুরুতে সব জমা হচ্ছে datastore অবজেক্টে।\n",
        "\n",
        "এখানে আমরা পুরো ডাটাসেটকে ‘ইটারেট’ মানে লুপে ফেলে জেসনের মধ্যে ফেলে দেবো, সেটা আমাদের দরকারি দুটো ভ্যালু পাইথন লিস্ট এর মধ্যে যোগ করে দিবে। ট্রেনিং এর অর্থ হচ্ছে আমাদের ফিচার অর্থাৎ ডাটার রেকর্ড এর সাথে লেভেল এর একটা সম্পর্ক বুঝতে পারা,  সেই কাজটা করার জন্য ‘সেন্টেন্স’ এবং ‘লেবেলস’ এর মধ্যে সম্পর্কটা বোঝার জন্য দুটো ডাটাকে আলাদা আলাদা করে দুটো লিস্টে রেখে দিচ্ছি আমরা। \n",
        "```\n",
        "sentences.append(item['content'])\n",
        "labels.append(item['category'])\n",
        "```\n",
        "এখানে জেসন থেকে আইটেম content এবং category থেকে datastore এ লুপ থেকে যোগ করছে।\n",
        "\n",
        "আমাদের টেক্সট এর জন্য যত ধরনের ‘প্রি-প্রসেসিং’ দরকার সেগুলো করার জন্য নিচের কোডগুলো দেখতে পারেন। \n",
        "\n",
        "ইউনিকোড এনকোডিং এর জন্য utf-8।\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaLaaqhNkUPd"
      },
      "source": [
        "import json\n",
        "with open('data_v2/data_v2.json', encoding='utf-8') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "for item in datastore:\n",
        "    sentences.append(item['content'])\n",
        "    labels.append(item['category'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEgvXCSLFWuN"
      },
      "source": [
        "ডাটাস্টোর এর প্রথম লিস্টটা আমরা দেখতে পারি। আমাদের কতগুলো বাক্য যোগ হয়েছে সেটার একটা লেন্থ এখানে দেখা যেতে পারে। এর পাশাপাশি কতগুলো ‘লেবেলস’ আছে সেটা দেখলে বোঝা যাবে মোট ৯টা ক্যাটাগরিতে আমাদের এই ডাটাগুলো ভাগ করা আছে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAu28l2WDUr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b80b10-f0a0-4842-b7a2-a66eef8d8d38"
      },
      "source": [
        "datastore[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'author': 'গাজীপুর প্রতিনিধি',\n",
              " 'category': 'bangladesh',\n",
              " 'category_bn': 'বাংলাদেশ',\n",
              " 'comment_count': 0,\n",
              " 'content': 'গাজীপুরের কালিয়াকৈর উপজেলার তেলিরচালা এলাকায় আজ বৃহস্পতিবার রাতের টিফিন খেয়ে একটি পোশাক কারখানার ৫০০ শ্রমিক অসুস্থ হয়ে পড়েছেন। এ ঘটনায় বিক্ষোভ করেছেন ওই কারখানার শ্রমিকেরা।সফিপুর মডার্ন হাসপাতালের জরুরি বিভাগের চিকিত্সক আল আমিন প্রথম আলো ডটকমকে বলেন, খাদ্যে বিষক্রিয়ায় তাঁরা (শ্রমিকেরা) অসুস্থ হয়ে পড়েছেন। এতে আতঙ্কিত হওয়ার কিছু নেই। অসুস্থদের চিকিত্সা দেওয়া হয়েছে।কারখানার শ্রমিক ও পুলিশ সূত্রে জানা যায়, উপজেলার তেলিরচালা এলাকার সেজাদ সোয়েটার লিমিটেড কারখানার শ্রমিকদের আজ রাত সাড়ে সাতটার দিকে টিফিন দেওয়া হয়। টিফিনে ছিল ডিম, রুটি, পেটিস ও কলা। টিফিন খেয়ে শ্রমিকেরা যথারীতি কাজে যোগ দেন। ওই টিফিন খাওয়ার প্রায় এক ঘণ্টা পর রাত সাড়ে আটটার দিকে কয়েকজন শ্রমিকের বমি ও পেট ব্যথা শুরু হয়। এরপর ধীরে ধীরে পুরো কারখানার শ্রমিকেরা অসুস্থ হতে থাকে। অনেকেই কারখানার মেঝেতে ঢলে পড়ে। এতে পাঁচ শতাধিক শ্রমিক অসুস্থ হয়ে পড়ে।পরে কারখানা কর্তৃপক্ষ দ্রুত যানবাহনের ব্যবস্থা করে তাদের সফিপুর জেনারেল হাসপাতাল, সফিপুর মডার্ন হাসপাতাল, উপজেলা স্বাস্থ্য কমপ্লেক্সসহ বিভিন্ন ক্লিনিকে ভর্তি করে। বাসি পচা খাবার দেওয়ায় শ্রমিকরা ক্ষুব্ধ হয়ে কারখানার সামনে বিক্ষোভ করে। খবর পেয়ে পুলিশ গিয়ে শ্রমিকদের বুঝিয়ে ও খাবার সরবরাহ প্রতিষ্ঠানের বিরুদ্ধে ব্যবস্থা নেওয়ার আশ্বাস দিলে শ্রমিকেরা শান্ত হয়।সফিপুর জেনারেল হাসপাতালে ভর্তি শ্রমিক জাকির হোসেন ও আসমা আক্তার বলেন, টিফিন খাওয়ার সময় ডিম ও কেক থেকে দুর্গন্ধ বের হচ্ছিল। এ কারণে অনেকেই ওই খাবার খায়নি। তবে বেশির ভাগ শ্রমিকই ওই খাবার খেয়েছে।কারখানার সহকারী উত্পাদন কর্মকর্তা (এপিএম) বছির উদ্দিন বলেন, টিফিনগুলি যে ঠিকাদারি প্রতিষ্ঠান কারখানায় সরবরাহ করে তাদের বিরুদ্ধে ব্যবস্থা নেওয়া হবে।মৌচাক পুলিশ ফাঁড়ির উপ-পরিদর্শক (এসআই) সৈয়দ আজহারুল ইসলাম প্রথম আলো ডটকমকে বলেন, শ্রমিকদের বুঝিয়ে শান্ত করা হয়েছে। এ ছাড়া কারখানা কর্তৃপক্ষকে খাদ্য সরবরাহ প্রতিষ্ঠানের বিরুদ্ধে ব্যবস্থা নিতে বলা হয়েছে।',\n",
              " 'modification_date': '০৪ জুলাই ২০১৩, ২৩:২৭',\n",
              " 'published_date': '০৪ জুলাই ২০১৩, ২৩:২৬',\n",
              " 'tag': ['গাজীপুর'],\n",
              " 'title': 'কালিয়াকৈরে টিফিন খেয়ে ৫০০ শ্রমিক অসুস্থ, বিক্ষোভ',\n",
              " 'url': 'http://www.prothom-alo.com/bangladesh/article/19030'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQl3n-CGkdtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf4aa62-4a53-40a2-c0dd-da300ac2b6b9"
      },
      "source": [
        "len(sentences)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "408471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "031qzxG_Rbxi"
      },
      "source": [
        "এখানে লেবেল হচ্ছে ৯টা। তবে মেশিন লার্নিং কী অক্ষর অর্থাৎ 'স্ট্রিং' বুঝবে? না বুঝবে না, সেকারণে এই লেবেলগুলোকে পাল্টে নেব সংখ্যায়। এখানে set কে ব্যবহার করছি এটার ইউনিক এলিমেন্টগুলোকে একটা একটা করে দেখতে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XD85i9YKnwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f43f4d-84a7-4c2b-be9d-da34bf0bb971"
      },
      "source": [
        "set(labels)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bangladesh',\n",
              " 'economy',\n",
              " 'education',\n",
              " 'entertainment',\n",
              " 'international',\n",
              " 'life-style',\n",
              " 'opinion',\n",
              " 'sports',\n",
              " 'technology'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QF3LN2cjmoO"
      },
      "source": [
        "## লেবেলগুলোকে সংখ্যায় নিয়ে আসি\n",
        "\n",
        "আমাদের এখানে যেহেতু লেবেলগুলো শব্দে আছে সে কারণে এটাকে সংখ্যায় কনভার্ট করে নেব কিছু টুলস অর্থাৎ সাইকিট-লার্নের কিছু প্রসেসিং টুল মাধ্যমে। আমরা এখানে সরাসরি পাইথন  অথবা টেন্সরফ্লো থেকে কিছু প্রসেসিং টুল ব্যবহার করতে পারতাম, তবে আমার কাছে মনে হয়েছে মেশিন লার্নিং এর প্রচুর টুল তৈরি করা আছে সাইকিট-লার্ন লাইব্রেরীতে। চেষ্টা করবো সেগুলোকে এখানে দেখাতে। এ মুহুর্তে আমরা sklearn.preprocessing.LabelEncoder ব্যবহার করবো যার কাজ হচ্ছে আমাদের লেবেলগুলোকে সংখ্যায় কনভার্ট করা যা শুরু হবে ০ দিয়ে এবং শেষ হবে আমাদের মোট লেবেলের সংখ্যা থেকে ১ বিয়োগ করে। অর্থাৎ ০ এর জন্য এখানে ১ বিয়োগ করছি। \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9MdJXDrjqQj",
        "outputId": "c8fcfc2e-651a-4c28-d721-3c9782970e28"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "testlabel = LabelEncoder()\n",
        "testlabel.fit([\"ঢাকা\", \"ঢাকা\", \"দিনাজপুর\", \"রংপুর\"])\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWzIKvYekneO",
        "outputId": "8f7e6525-d1a7-4457-e717-8caf8784c7fa"
      },
      "source": [
        "list(testlabel.classes_)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ঢাকা', 'দিনাজপুর', 'রংপুর']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc3tI4bhkgUH",
        "outputId": "65af81a1-8ac3-443c-9a64-10d3fed987bf"
      },
      "source": [
        "testlabel.transform([\"ঢাকা\", \"ঢাকা\", \"দিনাজপুর\", \"রংপুর\"])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3ar0l4xBZDh"
      },
      "source": [
        "উদাহরণ শেষ, আমাদের বাংলা নিউজ ডাটাসেটে ফিরে আসি। বাংলা নিউজ ডাটাসেটে labels হিসেবে আমাদের ৯টা ক্যাটেগরি ছিলো।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "papNKGF6tyV2"
      },
      "source": [
        "label = testlabel.fit_transform(labels)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw0CCNcHCK--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32112bca-2fb0-407d-a890-bf23f71f674b"
      },
      "source": [
        "set(label)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6, 7, 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH5gW6peOfHy"
      },
      "source": [
        "এটা এক ডাইমেনশনের অ্যারে। এখানে ভেক্টর দরকার আছে কী? বিশেষ করে ওয়ার্ড এমবেডিং এ লাগবে। হাতেকলমে পাইথন ডিপ লার্নিং বইটা দেখুন এই এমবেডিং এর ব্যাপারে।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8ypmtYp8HiK"
      },
      "source": [
        "## ‘মাল্টিক্লাস’ ক্লাসিফিকেশন এবং ‘লস’ ফাংশন\n",
        "\n",
        "(এটা মিস করা ঠিক হবে না)\n",
        "\n",
        "এধরনের যেকোনো ‘মাল্টিক্লাস’ অর্থাৎ (লেবেল ০-৮) ক্লাসিফিকেশন এর জন্য আমরা দুটো ধরনের ‘লস’ ফাংশন ব্যবহার করতে পারি। এবং এই দুটো ‘লস’ ফাংশন নেটওয়ার্কের প্রেডিকশন এর সাথে আসল লেবেলের সাথে মিলিয়ে দেখবে তার ফলাফল। এখানে আমরা (১) “ক্যাটাগরিক্যাল ক্রস এনট্রপি” অথবা (২) “স্পার্স ক্যাটাগরিক্যাল এনট্রপি” লস - যাই ব্যবহার করি না কেন, সেটার জন্য এই মুহূর্তে একটা সিদ্ধান্ত নিতে হবে। নিচে দেখুন আমাদের “ওয়ান হট এনকোডিং” ব্যবহার করতে হবে - যদি আমরা (১) “ক্যাটাগরিক্যাল ক্রস এনট্রপি” ব্যবহার করি।\n",
        " \n",
        "## “ক্যাটাগরিক্যাল ক্রস এনট্রপি” লস\n",
        "\n",
        "ধরা যাক, আমরা ‘এন’ সংখ্যক ক্লাসকে ধরে একটা ক্লাসিফিকেশন সমস্যা সমাধান করব - যার ক্লাসিফিকেশন অর্থাৎ ‘ফিচার এক্সট্রাকশন’ এর জন্য শুরুতে নিউরাল নেটওয়ার্ক ব্যবহার করলেও শেষের লেয়ারে ‘সফটম্যাক্স’ অ্যাক্টিভেশন ফাংশন ব্যবহার করব। সেখানে আমরা যদি “ক্যাটাগরিক্যাল ক্রস এনট্রপি” লস ব্যবহার করি তাহলে আমাদের ‘এন’ ডাইমেনশন ভেক্টরে সবগুলো সংখ্যা (০) শূন্য হবে - শুধুমাত্র, করেসপন্ডিং ক্লাসের জায়গায় সেটা  (১) ‘এক’ হয়ে যাবে। এটাকে ‘ওয়ান হট এনকোডিং’ বলছি। এটা ব্যবহার করলে আমাকে এখন নিচের OneHotEncoder ফাংশনটা ব্যবহার করতে হবে।\n",
        "\n",
        "যেমন: ৯টা (০-৮) ক্লাসের জন্য আমাদের ইনপুট যদি ক্লাস টু হয় তাহলে সেই ইনপুট এর জন্য লেভেল হতে পারে এরকম;\n",
        "০ ০ ১ ০ ০ ০ ০ ০ ০\n",
        "\n",
        "“ক্যাটাগরিক্যাল ক্রস এনট্রপি” লস যেকোনো ক্লাসিফিকেশন সমস্যার জন্য ব্যবহার করা যায়।\n",
        "\n",
        "## ‘স্পার্স ক্যাটেগরিক্যাল ক্রস এনট্রপি’ লস\n",
        "\n",
        "এর পাশাপাশি আমরা যদি ‘স্পার্স ক্যাটেগরিক্যাল ক্রস এনট্রপি’ ব্যবহার করি তাহলে সেখানে ‘এন’ ডাইমেনশনের ভেক্টর ব্যবহার না করে সরাসরি সংখ্যাটা বসিয়ে দিতে পারি। এখানে যেই ‘ইন্টেজার’ অর্থাৎ সংখ্যাটা ব্যবহার করা হবে সেটাই সেই ক্লাসের ডাটা হিসেবে ‘রিপ্রেজেন্ট’ করবে। সোজা ভাষায় বললে ‘স্পার্স ক্যাটেগরিক্যাল ক্রস এনট্রপি’ লসে সরাসরি সেই ক্লাসের জন্য যে সংখ্যাটা নির্ধারিত সেই সংখ্যাকে ব্যবহার করবে। এতে, কাজ কমে আসে।\n",
        "\n",
        "এখানে ‘স্পার্স ক্যাটেগরিক্যাল ক্রস এনট্রপি’ এর জন্য হবে সরাসরি সংখ্যা ২। \n",
        "\n",
        "## কখন ব্যবহার করা যাবে?\n",
        "\n",
        "“স্পার্স ক্যাটাগরিক্যাল এনট্রপি” লস তখনই ব্যবহার করা যাবে যখন প্রতিটা ইনপুট শুধুমাত্র একটা ক্লাসের জন্য প্রযোজ্য হবে। অর্থাৎ, কোন ইনপুট একসঙ্গে ২/৩ ক্লাসের জন্য ‘রিপ্রেজেন্ট’ করবে না। ফলে, এটা যদি আমরা ‘ওয়ান হট এনকোডিং’ ব্যবহার করি - তাহলে শুধুমাত্র সেই একটা এলিমেন্টে সংখ্যা (১) ‘এক’ বসবে আর বাকিগুলো সংখ্যা (০) শূন্য হবে যা আমাদের কম্পিউটেশনাল কাজের জন্য অনর্থক বটে। আমাদের কম্পিউটার রিসোর্স এর অপব্যবহার হবে - যখন বিশাল ডাইমেনশনে শুধুমাত্র সংখ্যা (০) শূন্য দিয়ে ভরে রেখেছি। যেখানে এই ভ্যালু দিয়ে আমাদের ‘রিপ্রেজেন্ট’ করার প্রয়োজন নেই। আর সে কারণে আমরা সেখানে সরাসরি একটা ‘ইন্টেজার’ ভ্যালু অর্থাৎ সংখ্যা ব্যবহার করছি।\n",
        "\n",
        "আচ্ছা তাহলে কী কথা রইল? আমরা এমুহুর্তে “ক্যাটাগরিক্যাল ক্রস এনট্রপি” লস ব্যবহার করলে নিচের “ওয়ান হট এনকোডিং” ব্যবহার করতে হবে। \n",
        "\n",
        "আর যদি না ব্যবহার করি? তাহলে কমেন্ট করে দিচ্ছি নিচের অংশটুকু, যাতে পরে ব্যবহার করতে পারেন। \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiaekWAovuOZ"
      },
      "source": [
        "## সাইকিট লার্নের 'ওয়ান হট এনকোডার' \n",
        "\n",
        "এ মুহুর্তে ব্যবহার করছি না।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NtSe4aIiHWj"
      },
      "source": [
        "\"\"\"\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "class_labels = label.reshape((label.shape[0], 1))\n",
        "cl_labels = encoder.fit_transform(class_labels)\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwUI4j7zZMCd"
      },
      "source": [
        "## ট্রেনিং এবং টেস্ট স্প্লিট করে নেই এখানে\n",
        "\n",
        "আমাদের 'সেন্টেনসেস' [len(sentences) = 408471] হচ্ছে প্রায় ৪ লক্ষ, ট্রেনিং সেট করে ফেলি ৩ লক্ষ ৫০ হাজারে। আপনারা পাল্টে নিন আপনাদের মতো করে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1sD-7v0kYWk"
      },
      "source": [
        "training_size = 350000\n",
        "\n",
        "training_sentences = sentences[0:training_size]\n",
        "testing_sentences = sentences[training_size:]\n",
        "training_labels = label[0:training_size]\n",
        "testing_labels = label[training_size:]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8O8o_UjkwYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22100560-2a3f-4d36-f622-0dfd789bcf18"
      },
      "source": [
        "len(testing_sentences)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU35R2A61xPB"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxomZKy2GBfm"
      },
      "source": [
        "## কর্পাসের প্রতিটা শব্দকে টোকেন বানানোর ধারণা\n",
        "\n",
        "আমাদের টেক্সট এর জন্য tokenizer.fit ফাংশনকে কল করছি যাতে আমাদের কনটেন্টের কর্পাসের প্রতিটা শব্দকে টোকেন হিসেবে তৈরি করতে পারে। এরপর আমরা সেগুলোকে একটা ‘ওয়ার্ড ইনডেক্সে’ দেখতে পারি। এখানে আমাদের প্রতিটা বাক্যকে একটা টোকেনের সিকোয়েন্স হিসেবে তৈরি করে দেবো, যাতে সেই বাক্যগুলোর প্রতিটা শব্দের একটা টোকেনের সিকোয়েন্স হিসেবে পাশাপাশি দেখাতে পারে। \n",
        "\n",
        "## নিউরাল নেটওয়ার্কে পাঠাতে দরকার 'প্যাডিং'\n",
        "\n",
        "আর যেহেতু আমাকে এটাকে নিউরাল নেটওয়ার্কে পাঠাতে হবে, সে কারণে প্রতিটা বাক্যকে ‘প্যাড’ ব্যবহার অর্থাৎ ‘প্যাডিং’ করে সবগুলোকে সংখ্যায় একই লেন্থে আনার ব্যবস্থা করব। যন্ত্র তো বোঝে না আমি বাক্য কোথায় শেষ করবো। আচ্ছা বলুনতো - আমরা মেপে মেপে কথা বলি? আমি আপনাদেরকে অনুরোধ করবো প্রতিটা অবজেক্টকে ‘প্রিন্ট’ করে দেখতে - যাতে একেকটা ‘ট্রানসফর্মেশন’ শব্দগুলো কিভাবে সংখ্যায় এবং তাদের সেই দৈর্ঘ্যগুলো কিভাবে পাল্টাচ্ছে সেটা দেখার জন্য।  এজন্য আমাদের পাইথনের প্রিন্ট কমান্ডই যথেষ্ট।\n",
        "\n",
        "## 'আউট অফ ভোকাবুলারি' শব্দগুলোকে ইনডেক্স ১ এ \n",
        "\n",
        "মনে আছে তো 'আউট অফ ভোকাবুলারি' শব্দগুলোর কথা? আমরা সেগুলোকে মিস করতে না।\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u8UB0MCkZ5N"
      },
      "source": [
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(training_sentences)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he_NukxosFm8"
      },
      "source": [
        "## পুরো ওয়ার্ড ইনডেক্স দেখা\n",
        "\n",
        "আমরা এখানে পুরো ওয়ার্ড ইনডেক্স দেখতে চাইলে tokenizer.word_index লিখলেই হবে। তবে আমি এখানে পুরোটা প্রিন্ট করলে ফাইল সাইজ বেড়ে যাবে। আর তাই, ডিকশনারি থেকে প্রথম ১০/২০টা ওয়ার্ড ইনডেক্স। লিস্টের শুরুতেই 'আউট অফ ভোকাবুলারি' শব্দগুলোর জন্য ১। শুরুর দিকে সবচেয়ে বেশি ব্যবহারিত 'স্টপ' ওয়ার্ড যেগুলো ফেলে দেয়া যায়। এখানে কমপ্লেক্সিটি এড়ানোর জন্য ফেলছি না।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gt0bCdy8FYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb566b2-9f1f-4578-fabb-ab9e4b358ca5"
      },
      "source": [
        "list(tokenizer.word_index.items())[:20] "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<OOV>', 1),\n",
              " ('ও', 2),\n",
              " ('এ', 3),\n",
              " ('থেকে', 4),\n",
              " ('করে', 5),\n",
              " ('করা', 6),\n",
              " ('বলেন', 7),\n",
              " ('এই', 8),\n",
              " ('জন্য', 9),\n",
              " ('না', 10),\n",
              " ('তিনি', 11),\n",
              " ('সঙ্গে', 12),\n",
              " ('তাঁর', 13),\n",
              " ('এক', 14),\n",
              " ('একটি', 15),\n",
              " ('নিয়ে', 16),\n",
              " ('এবং', 17),\n",
              " ('করতে', 18),\n",
              " ('হয়।', 19),\n",
              " ('মধ্যে', 20)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV9OfSkxtlCO"
      },
      "source": [
        "## ভোকাবুলারি সাইজ ঠিক করে নিচ্ছি\n",
        "\n",
        "ওয়ার্ড ইনডেক্স এর সাথে ইনডেক্স ০ এর জন্য ১ যোগ করে দিচ্ছি। পরের লাইনে সাইজ পেয়ে যাচ্ছি।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vessG1_5WoF"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67mga5M98xVK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18d07603-ab39-4953-a0dd-b0281c68f880"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2040686"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSA7q68g1eXM"
      },
      "source": [
        "## প্যাডিং, এখানে ম্যাক্সিমাম লেনথ ৫০০ শব্দ\n",
        "\n",
        "প্রতিটা বাক্য যতো ছোট অথবা বড় হোক, সেটাকে কেঁটে অথবা প্যাডিং দিয়ে বড় করে ৫০০ শব্দের ভেতরে রাখবে। সবকিছু একরকমের ধারনায় রাখার জন্য আমরা trunc_type এবং\n",
        "padding_type দুটোকেই 'পোস্ট' অর্থাৎ 'প্যাডিং' শেষের দিকে রাখছি।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeBFl68g7g2k"
      },
      "source": [
        "max_length = 500\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nE33e4ApSr7"
      },
      "source": [
        "## টেক্সট টু সিকোয়েন্স একটু দেখি\n",
        "\n",
        "একটু টেস্ট করে দেখি জিনিসটা কাজ করে কিনা? একটা জিনিস লক্ষ্য করেছেন - সবচেয়ে বেশি ব্যবহার হওয়া শব্দগুলোর সিরিয়াল আগের দিকে? কেন?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSoq_JwEpDcq",
        "outputId": "5975ba39-e66a-496e-b42a-ad0e53302a64"
      },
      "source": [
        "tokenizer.texts_to_sequences(['ভোকাবুলারি সাইজ ঠিক করে নিচ্ছি'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[178189, 34714, 413, 5, 18478]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-MeZoF_mkfy"
      },
      "source": [
        "## প্রথম ট্রেনিং সিকোয়েন্স প্রিন্ট করে দেখি\n",
        "\n",
        "শব্দগুলোকে মেশিন চেনে এইভাবে। সংখ্যায়।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSp1Z1u9lRSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "578b595f-90e2-4229-98b1-1e3c6ed16c1e"
      },
      "source": [
        "print(training_sequences[0])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2259, 15036, 78, 72533, 687, 61, 215, 2489, 18955, 8167, 15, 1074, 1368, 1046, 620, 1697, 294, 10719, 3, 966, 1270, 114, 21, 1368, 690932, 17037, 598, 1134, 201, 8588, 414, 1254, 27, 795, 5183, 7, 11270, 57451, 62, 2185, 1697, 294, 10719, 102, 7279, 909, 70, 150, 93383, 8308, 316, 186860, 620, 2, 52, 358, 131, 721, 78, 72533, 191, 302937, 39067, 2420, 1368, 1350, 61, 253, 680, 2990, 66, 18955, 316, 166, 81996, 54, 3394, 9610, 124015, 2, 81997, 18955, 8167, 2185, 9313, 468, 724, 240, 21, 18955, 10238, 692, 14, 662, 24, 253, 680, 1791, 66, 1919, 3591, 9453, 2, 8055, 3248, 49, 166, 225, 1768, 1768, 806, 1368, 2185, 1697, 96, 536, 1037, 1368, 7597, 9454, 2517, 102, 224, 1189, 620, 1697, 294, 175688, 1530, 760, 522, 3950, 227, 5, 94, 23802, 727, 995, 23802, 17037, 995, 87, 445, 40841, 55, 5429, 373, 116, 20612, 10856, 884, 6377, 113308, 2374, 294, 1368, 213, 1270, 116, 276, 1762, 52, 955, 1350, 12142, 2, 884, 1113, 706, 97, 227, 1613, 2430, 850, 2185, 3375, 690933, 727, 247, 373, 620, 2088, 126, 2, 10421, 942, 7, 18955, 10238, 271, 3394, 2, 6172, 4, 8872, 334, 3750, 3, 101, 1037, 21, 884, 265414, 38, 830, 594, 50081, 21, 884, 690934, 339, 12744, 68, 265415, 60461, 308, 7, 690935, 36, 4386, 501, 7044, 1113, 5, 94, 97, 227, 829, 458093, 52, 8643, 5954, 1440, 863, 3264, 10840, 83, 27, 795, 5183, 7, 1350, 12142, 3375, 6, 217, 3, 434, 1530, 3137, 1552, 1113, 706, 97, 227, 206, 121, 217]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrAlWBKf99Ya"
      },
      "source": [
        "# টেন্সরফ্লো ২.x এর জন্য এই কনভার্সন\n",
        "import numpy as np\n",
        "\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LagJyWBNwkYU"
      },
      "source": [
        "## কম ডাইমেনশন দিয়ে শুরু করা"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eJSTTYnkJQd"
      },
      "source": [
        "embedding_dim = 8\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1ybowrMQiy9"
      },
      "source": [
        "## ট্রেনিং ডাটা, প্যাডিং করা সহ\n",
        "\n",
        "এখানে যেহেতু আমাদের প্রতিটা বাক্য একই সমান নয়, তাই এই ব্যবস্থা। মানুষ কি মেপে মেপে কথা বলতে পারে? পারে না। সে কারণে আমরা যখন নিউরাল নেটওয়ার্কে বাক্যগুলোকে সিকোয়েন্স হিসাবে পাঠাবো - তখন এই জিনিসগুলোতে ‘প্যাডিং’ ব্যবহার করে সবগুলো বাক্য’র দৈর্ঘ্য এক সমান  রাখবো। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAevE2nzll_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d91ed4-3c5c-42da-8797-db258fd4fec4"
      },
      "source": [
        "training_padded"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2259,  15036,     78, ...,      0,      0,      0],\n",
              "       [  1082, 216926,  21406, ...,      0,      0,      0],\n",
              "       [   764,   2002,    470, ...,      0,      0,      0],\n",
              "       ...,\n",
              "       [ 22488,  39163,  15195, ...,      0,      0,      0],\n",
              "       [  1692,    174,   9581, ...,      0,      0,      0],\n",
              "       [ 23523,  60309,    558, ...,      0,      0,      0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn72KM4EQw1l"
      },
      "source": [
        "## শব্দের এমবেডিং এর ধারণা\n",
        "\n",
        "‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’ এ এমবেডিং লেয়ার নিয়ে অনেক কথা বলেছি ‘হাতেকলমে পাইথন ডিপ লার্নিং’ বইটাতে। বিশেষ করে, এনএলপি, মেশিন লার্নিং এর একটা বড় অংশ হচ্ছে কিভাবে শব্দগুলোকে ঠিকমতো ‘রিপ্রেজেন্ট’ করা যায় - যাতে সেগুলোর মধ্যে কানেকশন অর্থাৎ সম্পর্কগুলোকে আন্দাজ করা যায়। ওয়ার্ড অর্থাৎ শব্দের এম্বেডিং আমাদের একটা বেশ দক্ষ ‘ডেন্স’ (নিউরালের ডেন্স) রিপ্রেজেন্টেশন তৈরি করে দেয়, যেখানে কাছাকাছি অর্থাৎ সম্পর্কিত শব্দগুলোর একই ধরনের এনকোডিং হয়। মেশিন লার্নিং বা ডিপ-লার্নিং এ আমরা যেহেতু নিজেদের হাতে এই শব্দগুলোকে এনকোডিং করিনা - সে কারণে এই শব্দের এমবেডিং লেয়ার একটা ‘ডেন্স’ ভেক্টরের একটা ‘লুকআপ টেবিল’ তৈরি করে দেয়, যার মধ্যে ফ্লোটিং পয়েন্ট ভ্যালুগুলোকে কাছাকাছি রাখে। \n",
        "\n",
        "এখানে ফ্লোটিং পয়েন্ট ভ্যালুগুলো ভেক্টরগুলোর দৈর্ঘ্যগুলোকে কিছু প্যারামিটার এর মাধ্যমে ঠিক করে দেয় ট্রেনিং এর সময়ে। এই শব্দের এম্বেডিং এর ভ্যালুগুলোকে ম্যানুয়ালি নির্ধারণ না করে এগুলোকে ‘ট্রেনিং প্যারামিটার’ হিসেবে শুরুতে দেয়া হলেও মডেলের ট্রেনিং এর সময় ওয়েটগুলো আস্তে আস্তে শিখতে থাকে - তাদের আসল ভ্যালু কত হতে পারে। সাধারণ নিউরাল নেটওয়ার্ক এর ‘ডেন্স’ নেটওয়ার্ক এর মতো একটা মডেল যখন নিজেদের ওয়েটগুলোকে টিউন করে নেয় ট্রেনিং এর সাথে সাথে, সেভাবে এই শব্দের এম্বেডিংগুলো ভেক্টর স্পেসে ‘টিউন’ করে নিতে পারে। ছোট ডাটাসেটের ক্ষেত্রে - এ ধরনের শব্দের এম্বেডিং ৮ ডাইমেনশনাল দিয়ে শুরু হলেও দরকার পড়লে ১০২৪ ডাইমেনশনের কাজও আসতে পারে বড় ডাটাসেটে। বড় ডাইমেনশনের এমবেডিং - এ শব্দগুলোর মধ্যে আরও সূক্ষ্ম সম্পর্কগুলোকে ধরতে পারে - তবে এগুলো ট্রেইন করতে প্রচুর সময় চলে যায়।\n",
        "\n",
        "## এমবেডিং লেয়ারের কাজ\n",
        "\n",
        "এই এমবেডিং লেয়ারের ডাইমেনশালিটি অর্থাৎ তার ‘প্রস্থ’ নিউরাল নেটওয়ার্ক এর নিউরন এর সাথে তুলনা করতে পারেন - যা আমরা দেখেছি ‘ডেন্স’ লেয়ার এর ক্ষেত্রে। \n",
        "\n",
        "যেকোনো নিউরাল নেটওয়ার্ক এর মতো এই এম্বেডিং লেয়ারে ওয়েটগুলো দৈবচয়নের ভিত্তিতে ‘ইনিশিয়ালাইজ’ করা হয়। পরবর্তীতে, ট্রেনিং এর সময় ‘লস’ ক্যালকুলেশন করে এর ফ্লোটিং পয়েন্ট ভ্যালুগুলো আস্তে আস্তে তার আসল মূল্যমানে ফেরত আসতে থাকে। এই পদ্ধতিকে আমরা ‘ব্যাক-প্রপাগেশন’ বলি। ট্রেনিং শেষে এম্বেডিংগুলো সম্পর্কিত শব্দগুলোর মত করে এর কাছাকাছি এনকোডিং এর জন্য পাশাপাশি চলে আসে। \n",
        "\n",
        "১০০০ শব্দের ভোকাবুলারিকে ৫ ডাইমেনশনে এমবেড করতে চাইলে এরকম বলতে পারি।\n",
        "\n",
        "`embedding_layer = tf.keras.layers.Embedding(1000, 5)`\n",
        "\n",
        "## দুটো লস নিয়ে কাজ করে দেখুন\n",
        "\n",
        "(আগে আলাপ করেছি)\n",
        "\n",
        "১. “ক্যাটাগরিক্যাল ক্রস এনট্রপি” লস\n",
        "\n",
        "২. ‘স্পার্স ক্যাটেগরিক্যাল ক্রস এনট্রপি’ লস\n",
        "\n",
        "## মডেলটা দেখুন, সাধারণ সিকোয়েন্সিয়াল লেয়ার\n",
        "\n",
        "এখানে আমাদের ইনপুট ডাইমেনশনে ভোকাবুলারি সাইজ দিয়েছি। এখানে input_dim: ইন্টেজার, অর্থাৎ ভোকাবুলারির সাইজ = ওয়ার্ড ইনডেক্সের সাইজ + 1 এবং \n",
        "output_dim: এটাও ইন্টেজার, আমাদের ডেন্স এমবেডিং এর ডাইমেনশন হবে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FufaT4vlkiDE"
      },
      "source": [
        "# model = tf.keras.Sequential([\n",
        "#    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "#    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "#    tf.keras.layers.Dense(128, activation='relu'),\n",
        "#    tf.keras.layers.Dense(9, activation='softmax')\n",
        "#])\n",
        "\n",
        "model =  tf.keras.Sequential([\n",
        "                              tf.keras.layers.Embedding(input_dim=vocab_size, \n",
        "                              output_dim=embedding_dim, \n",
        "                              input_length=max_length),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
        "                              tf.keras.layers.Dense(9,  activation='softmax')\n",
        "])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho9QlxWRR-mA"
      },
      "source": [
        "## দুটো 'লস' পদ্ধতি দিয়ে চেষ্টা করা \n",
        "\n",
        "আমরা যদি আগে 'ওয়ান হট এনকোডিং' ব্যবহার করতাম, তাহলে নিচেরটা চালিয়ে দেখুন। এখানে 'categorical_crossentropy' ব্যবহার করতাম যদি আগে 'ওয়ান হট এনকোডিং' দিকে অনেকগুলো ভেক্টর তৈরি করতাম। তবে কম্পিউটেশনাল প্রসেসিং কমাতে আমরা 'sparse_categorical_crossentropy' ব্যবহার করছি যাতে একই কাজ দ্রুত করা যায়।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVkIm0MWxt6b"
      },
      "source": [
        "# model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iouAcB9DR7pV"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtJefqM7Eu58"
      },
      "source": [
        "## মডেল সামারি\n",
        "\n",
        "শুরুতেই এমবেডিং লেয়ার, এমবেডিং ডাইমেনশন ৮, ইনপুট লেনথ ৫০০, এর মধ্যে বাই ডাইরেকশনাল 'এলএসটিএম' যা নিয়ে কথা বলছি সামনে। সব শেষের লেয়ারে ডেন্স যা সফটম্যাক্স এক্টিভেশন দিয়ে ০-৮ অর্থাৎ ৯ টা ক্যাটাগরিতে মাল্টিক্লাস ক্লাসিফিকেশন করবে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfDt1hmYkiys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e9c738d-7b5b-4742-9c1c-624a540927af"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 500, 8)            16325488  \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 500, 256)          140288    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 256)               394240    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 9)                 2313      \n",
            "=================================================================\n",
            "Total params: 16,862,329\n",
            "Trainable params: 16,862,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pHOkef2_dnD"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwRUT3oDxkj2"
      },
      "source": [
        "## ট্রেনিংয়ের সময় নিদেনপক্ষে ৬ ঘন্টা\n",
        "\n",
        "গুগল কোলাবের বিনামূল্যের ‘জিপিইউ’ এবং ‘টিপিইউ’ ব্যবহার করে যতটুকু পাওয়া যায়। কারো বাসায় গেমিং পিসি থাকলে, সেখানে ‘জিপিইউ’ দিয়ে এই সময়টা কিছুটা কমিয়ে আনা যেতে পারে। অনেক ডাইমেনশন, তাই সময় লাগছে বেশি।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DTKQFf1kkyc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2e14d75-3010-4db7-a460-bc966c245691"
      },
      "source": [
        "num_epochs=1\n",
        "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), callbacks=[tensorboard_callback], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPPEk34of5ip"
      },
      "source": [
        "## মডেলকে সেভ করি\n",
        "\n",
        "মডেলের ট্রেনিংয়ে সময় বেশি লাগে বলে এ ধরনের ট্রেনিং এর পরে মডেলগুলোকে ‘গুগল ড্রাইভে’ ‘সেভ’ করে রাখা ভালো। পুরো মডেলের ‘ওয়েট’ এবং ‘আর্কিটেকচার’ ধরে ‘সেভ’ করে রাখলে কাজে সুবিধা হয়। তবে. চেক-পয়েন্ট ধরে ‘ওয়েট’গুলোকে আলাদা আলাদা করে ‘সেভ’ করে রাখতে পারলে পরে আরো ভালো সুবিধা পাওয়া যায়।\n",
        "\n",
        "কেরাসের মডেল.সেভ দিয়ে এমুহুর্তে স্টোর করে রাখলাম। এর আগে গুগল ড্রাইভকে লোড করে নিতে হবে। এখানে একটা অথেনটিকেশন চাইবে, যা দিলে আপনার কাজ কমে যাবে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1P-h1Fsh5XB"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99pEwc9BgBDk"
      },
      "source": [
        "model.save('/content/gdrive/My Drive/Colab Notebooks/model1epoch.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW5Whf0jKbB7"
      },
      "source": [
        "## অ্যাক্যুরেসি এবং লসকে প্লট করি\n",
        "\n",
        "আমাদের মডেলের ম্যাট্রিক্স হিসেবে 'অ্যাক্যুরেসি' এবং লসের হিসেব আগের হিস্টরির মধ্যে জমা থাকলেও সেটাকে ভিজ্যুয়ালাইজেশন না করলে মডেলের পারফরমেন্স বোঝা যাবে না। আমরা প্লট করছি ইপক দিয়ে \"accuracy\" এবং \"loss\" এর আঙ্গিকে। এখানে ট্রেনিং অ্যাক্যুরেসি বাড়লেও ভ্যালিডেশন অ্যাক্যুরেসি সেভাবে হয়নি। এখানে কী হচ্ছে তাহলে? ওভারফিটিং না আন্ডারফিটিং?\n",
        "\n",
        "আমাদের টেন্সরবোর্ড দিয়ে দেখলে নিচের এই কোড লাগে না আর! সামনে দেখুন উদাহরনটা।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HYfBKXjkmU8"
      },
      "source": [
        "## অ্যাক্যুরেসি এবং লস\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuakxVbM1y7t"
      },
      "source": [
        "## টেন্সরবোর্ড দিয়ে প্লটিং\n",
        "\n",
        "আমরা সারাজীবন ‘ম্যাটপ্লটলিব’ এই কাজে ব্যবহার করলেও এখন টেন্সরবোর্ড কোন ধরনের কোড ছাড়াই অ্যাক্যুরেসি এবং লসকে প্লট এবং ট্র্যাকিং করে দেখাচ্ছে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALtAh06X_CHc"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe3nYQjiYYAr"
      },
      "source": [
        "## তিনটা উদাহরণ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG8-ArY-qDcz"
      },
      "source": [
        "## আমি বেশি লুপ ব্যবহার করতে চাইনা বোঝানোর সময়\n",
        "\n",
        "sentence = [\"কৃত্রিম সংকট তৈরি করে ঠাকুরগাঁওয়ে ডিলারদের বিরুদ্ধে ট্রিপল সুপার ফসফেট (টিএসপি) সার অনেক বেশি দামে বিক্রির অভিযোগ উঠেছে। কৃষকদের কাছ থেকে সারের দাম বেশি নেওয়া হলেও ডিলাররা কোনো পাকা রসিদ (ভাউচার) দিচ্ছেন না। এতে কারও কাছে অভিযোগও দিতে পারছেন না কৃষকেরা।\"]\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(model.predict(padded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vKl8TWqzgFE"
      },
      "source": [
        "sentence = [\"করোনা পরীক্ষা করে রেস্তোরাঁয় বসে খাওয়া যাবে। যুক্তরাষ্ট্রের নিউইয়র্ক নগরীর একটি রেস্তোরাঁ এমন ব্যবস্থা শুরু করছে। দরজার বাইরে গ্রাহকদের তাৎক্ষণিকভাবে করোনা পরীক্ষা করা হবে। রেস্তোরাঁর পক্ষ থেকে প্রশিক্ষণপ্রাপ্ত একজন পরীক্ষক থাকবেন। তিনি গ্রাহক ও কর্মীদের নমুনা সংগ্রহ করে পরীক্ষা করবেন। ফলাফলের অপেক্ষায় থাকা গ্রাহকদের বিনা মূল্যে পানীয় সরবরাহ করা হবে।\"]\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(model.predict(padded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4KM7ilP0Qos"
      },
      "source": [
        "## আমি বেশি লুপ ব্যবহার করতে চাইনা বোঝানোর সময়\n",
        "\n",
        "sentence = [\"লালমনিরহাটের পাটগ্রাম উপজেলায় আবু ইউনুছ মোহাম্মদ শহীদুন্নবী জুয়েলকে পিটিয়ে ও পুড়িয়ে হত্যার ঘটনায় হওয়া মামলায় আরও পাঁচজনকে গ্রেপ্তার করেছে পুলিশ। গত মঙ্গলবার রাতে তাঁদের ধরা হয়। আজ বুধবার আদালতের মাধ্যমে তাঁদের কারাগারে পাঠিয়েছে পুলিশ।\"]\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(model.predict(padded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M36jEKaCsO9"
      },
      "source": [
        "## ওয়ার্ড এমবেডিংগুলোকে ডিস্কে সেভ করি\n",
        "\n",
        "আমাদের ট্রেনিং এর সময়ে যেই ওয়ার্ড এমবেডিংগুলো শিখেছি, সেগুলোকে সেভ করবো পরে এমবেডিং প্রজেক্টরে। সে এক অসাধারণ অভিজ্ঞতা! এখানে এমবেডিংগুলো মডেলের এমবেডিং লেয়ারের 'ওয়েট' হিসেবে থাকে। এখানে ওয়েটের ম্যাট্রিক্স এর শেপ (vocab_size, embedding_dimension) হিসেবে ধরে নেই।\n",
        "\n",
        "এখানে মডেলের  get_layer() এবং get_weights() থেকে ওয়েট পাওয়া যাবে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG14gwgO0DTN"
      },
      "source": [
        "weights = model.get_layer('embedding').get_weights()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBVANdJTdJSW"
      },
      "source": [
        "## ভোকাবুলারি সাইজ এবং এমবেডিং ডাইমেনশন\n",
        "\n",
        "মডেলের লেয়ার ধরে আমরা শুরুর ইনপুট লেয়ার ধরে vocab_size, embedding_dim এর শেপ করার চেষ্টা করতে পারি।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cccpP-BICpXI"
      },
      "source": [
        "# শেপ দেখুন (vocab_size, embedding_dim)\n",
        "print(weights.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMWyh-VBjIQG"
      },
      "source": [
        "## শব্দকে ‘ভিজুয়ালাইজেশন’ - অনেক ডাইমেনশনে\n",
        "\n",
        "আমাদের প্রতিটা শব্দ - ধরুন, বাংলা শব্দকে সংখ্যায় নিয়ে যাবার আগে সেটাকে টোকেনে ভাগ করে ফেলি। মানুষ এই শব্দগুলোকে ঠিকমতো বুঝতে চাইলে সেটাকে প্লট করতে হবে। আমরা মেশিন লার্নিং এর যেকোনো জিনিস বুঝতে চাইলে সেটাকে শুরুতে ‘ভিজুয়ালাইজ’ করার জন্য আমরা দুই ডাইমেনশনে প্লট করি। \n",
        "\n",
        "মানুষের মাথা - বিশেষ করে মাথার ‘নিউরাল নেটওয়ার্ক’ এতটাই চমৎকার যে - যে কোনো ডাটাকে ঠিকমতো প্লটিং করতে পারলে সেটার মধ্যে প্যাটার্ন খুঁজে বের করে ফেলবে আমাদের অসাধারণ মাথা। তবে সেই ডাটা অনেক মানে অনেক বেশি হলে খালি চোখে সেটা কয়েক ডাইমেনশনে দেখা কষ্টকর।\n",
        "\n",
        "তবে, এই শব্দগুলোকে সংখ্যায় পরিবর্তন করলে সেগুলোকে অনেকগুলো ডাইমেনশনে অর্থাৎ শতাধিক এক্সিসে প্লট করলে সম্পর্কের ভালো ধারণা পাওয়া যায়। সেখানে প্রতিটা শব্দের সাথে আরেকটা শব্দের সম্পর্ক, কে কার কাছে এবং দুরে - অথবা কে কার থেকে কতো দুরে, আপেক্ষিক এবং আসল কাছে দুরের ধারণা থেকে অসাধারণ প্রজ্ঞা পাওয়া যায়। ধরুন, গত ২০ বছরের পুরো বাংলাদেশের নামকরা বাংলা খবরের কাগজের প্রতিটা শব্দকে আমরা যদি ১২৮ ডাইমেনশনে (যতো বেশি, ততো বেশি প্রসেসিং ক্ষমতা দরকার) প্লট করি, তখন সেই শব্দগুলোর মধ্যে কোন শব্দগুলো বারবার ঘুরে ফিরে আসছে এবং কোন শব্দগুলো আরেকটা শব্দের সাথে প্রথাগত সম্পর্কিত না হলেও সেখানে ওই ঘটনার জন্য একটা সম্পর্কের হিসেব মানুষকে অন্যভাবে চিন্তা করতে বাধ্য করছে।\n",
        "\n",
        "## শব্দের এমবেডিংগুলোকে ‘ভিজুয়ালাইজেশন’\n",
        "\n",
        "শব্দের এমবেডিংগুলোকে ‘ভিজুয়ালাইজ’ করতে হলে আমাদের vecs.tsv এবং meta.tsv ফাইলদুটোকে আপলোড করতে হবে এমবেডিং প্রজেক্টরে। আমরা চলে যাই এমবেডিং প্রজেক্টরে উপরের লিংক দিয়ে। এটা লোকাল ‘টেন্সরবোর্ড’ ইন্সট্যান্স দিয়ে চালানো সম্ভব। তবে, অনেক সময়ে লোকাল কম্পিউটারে না করে এটা আপলোড করি টেন্সরফ্লো প্রজেক্টরে। ‘লোড ডাটা’ ক্লিক করে আপলোড করে দেই দুটো ফাইল। \n",
        "\n",
        "এখন এই এমবেডিং যেগুলোকে আমরা ট্রেনিং করেছিলাম, সেগুলো ভিজুয়ালাইজেশন এ আসবে। আমাদের এখানে শব্দগুলোর সাথে যেগুলো কাছাকাছি শব্দ সেগুলোকে দেখা যাবে এর কাছাকাছি। কিভাবে একটা শব্দ আরেকটা শব্দের সাথে সম্পর্কযুক্ত সেটা বোঝা যাবে এই এমবেডিং প্রজেক্টরে। যেকোনো একটা শব্দ লিখে সার্চ দিন, আপনি পেয়ে যাবেন ওই শব্দগুলোর সাথে সম্পর্কিত শব্দগুলো কি কি হতে পারে। এটা একটা মজার খেলা।\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoBXVffknldU"
      },
      "source": [
        "## প্রজেক্টরে দেখানোর জন্য গুগল কোলাব অথবা নিজের মেশিনে 'সেভ' করে রাখি\n",
        "\n",
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for word_num in range(1, vocab_size):\n",
        "  word = reverse_word_index[word_num]\n",
        "  embeddings = weights[word_num]\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uTh38_9ekrG"
      },
      "source": [
        "## এমবেডিং প্রজেক্টর\n",
        "\n",
        "এমবেডিং প্রজেক্টরে দেখানোর জন্য ভেক্টর এবং মেটা ফাইলের ট্যাব সেপারেটেড ভ্যালু হিসেবে ডাউনলোড করে নেব।\n",
        "\n",
        "ওয়েটগুলোকে ডিস্কে 'সেভ' করে রাখি। আমাদের এমবেডিং প্রজেক্টরে (http://projector.tensorflow.org/) এই দুটো ট্যাব সেপারেটেড ফরম্যাট ফাইল আপলোড করলেই দেখা যাবে হাই ডাইমেনশনাল ডাটাগুলো কিভাবে একে অপরের সাথে সম্পর্কিত। একটা ভেক্টরের ফাইল, (যার ভেতরে এমবেডিংগুলো থাকছে) আরেকটাতে মেটাডাটা (যার ভেতরে শব্দগুলো থাকছে) হলেই চলবে।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTagw23ONvlF"
      },
      "source": [
        "## ডাউনলোড করে নেই ফাইল দুটো\n",
        "\n",
        "গুগল কোলাবে এই নোটবুকটা চালালে প্রথম ফাইল সয়ংক্রিয় ভাবে ডাউনলোডের হয়ে যাবে। পরেরটার জন্য আপনার পারমিশন চাইবে। আর সেটা মিস করলে বামের ফাইল ব্রাউজার দিয়ে নামিয়ে নিতে পারেন। অথবা 'ভিউ' থেকে  টেবিল অফ কনটেন্ট' দিয়ে নামিয়ে নিতে পারেন। লোকাল মেশিন হলে এমনিতেই পেয়ে যাচ্ছেন। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4eZ5HtVnnEE"
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upRKcdEagi7R"
      },
      "source": [
        "## কেরাস মডেলকে আবার লোড করে দেখি\n",
        "\n",
        "ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’য়ে মডেলগুলো ট্রেইন করতে এতো বেশি সময় লাগে যা অনেকের ধৈর্যচ্যুতি ঘটাতে পারে। আর সে কারণেই এই মডেলগুলোকে ‘সেভ’ করে রাখা। এখন মডেলটাকে লোড করে ‘আর্কিটেকচার’ বোঝার চেষ্টা করি। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baG6elUJghD-"
      },
      "source": [
        "new_model = tf.keras.models.load_model('/content/gdrive/My Drive/Colab Notebooks/model.h5')\n",
        "\n",
        "# সেটার আর্কিটেকচার দেখি\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}